{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import *\n",
    "from scapy.layers.l2 import Ether\n",
    "from scapy.layers.inet import IP, UDP, TCP\n",
    "from scapy.contrib.igmp import IGMP\n",
    "from scapy.utils import PcapWriter\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy specific for ip addresses in dict format\n",
    "from math import log2\n",
    "from scipy.special import gamma\n",
    "def calculate_entropy(data_dict):\n",
    "    total_count = sum(data_dict.values())\n",
    "    \n",
    "    if total_count == 0:\n",
    "        return 0.0  # Avoid division by zero\n",
    "    \n",
    "    probabilities = [count / total_count for count in data_dict.values()]\n",
    "    \n",
    "    entropy = -sum(p * log2(p) for p in probabilities if p > 0)\n",
    "    # normalize entropy\n",
    "    entropy /= log2(total_count)\n",
    "    return entropy\n",
    "\n",
    "# Weibull parameters\n",
    "def calculate_weibeull_paramter(data_dict):\n",
    "    ip_f0_values = np.array(len(list(data_dict.keys())))\n",
    "    ip_f1_values = np.array(sum(data_dict.values()))\n",
    "    ip_f2_values = np.array(sum([count ** 2 for count in list(data_dict.values())]))\n",
    "   \n",
    "    # formula\n",
    "    ip_M2 = ip_f2_values / ip_f0_values\n",
    "    ip_M1 = ip_f1_values / ip_f0_values\n",
    "    ip_freqm_est_mean = ip_M1\n",
    "    ip_freqm_est_var = ip_M2 - ip_M1**2\n",
    "    ip_weibull_k = (ip_freqm_est_mean / (ip_freqm_est_var)**0.5) ** (1.086)\n",
    "    ip_weibull_theta = ip_freqm_est_mean / gamma(1 + 1 / ip_weibull_k)\n",
    "    return ip_weibull_k, ip_weibull_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP.payload_guess = []  # speed up, we don't need payload info\n",
    "pkt_pcap = PcapReader('./trace/Merged_MAWI19040918+CAIDAdual_oneway.pcap')\n",
    "pkt_pcap_filename = pkt_pcap.filename.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sip_ct = Counter()\n",
    "dip_ct = Counter()\n",
    "sip_pkt_size_ct = Counter()\n",
    "dip_pkt_size_ct = Counter()\n",
    "# Basic features\n",
    "total_packet_count = 0\n",
    "total_packet_size = 0 \n",
    "# F0, F1, F2 values\n",
    "sip_F0_values, sip_F1_values, sip_F2_values = [], [], []\n",
    "dip_F0_values, dip_F1_values, dip_F2_values = [], [], []\n",
    "# Statistical values\n",
    "sip_raw_mean, sip_raw_std, sip_raw_skew, sip_raw_kurtosis = [], [], [], []\n",
    "# Entropy values\n",
    "entropy_sip, entropy_dip = [], []\n",
    "# Weibull parameters\n",
    "sip_weibull_k, sip_weibull_theta = [], []\n",
    "dip_weibull_k, dip_weibull_theta = [], []\n",
    "\n",
    "time_interval = 1.0 # second\n",
    "start_time = None\n",
    "pkt_index = 0\n",
    "for pkt in pkt_pcap:\n",
    "    if pkt_index == 0:\n",
    "        start_time = pkt.time\n",
    "    current_time = pkt.time\n",
    "    pkt_index += 1\n",
    "    if IP in pkt:\n",
    "        sip_ct.update([pkt[IP].src])\n",
    "        dip_ct.update([pkt[IP].dst])\n",
    "        sip_pkt_size_ct.update({pkt[IP].src: pkt[IP].len})\n",
    "        dip_pkt_size_ct.update({pkt[IP].dst: pkt[IP].len})\n",
    "        total_packet_count += 1\n",
    "        #total_packet_size += pkt[IP].len\n",
    "        if (current_time-start_time) >= time_interval:\n",
    "            # store path\n",
    "            path = './results/{}/{}s'.format(pkt_pcap_filename, time_interval)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            \n",
    "            \n",
    "            sip_df = pd.concat([pd.DataFrame.from_dict(sip_ct, orient='index', columns=['sip_ct']),\n",
    "                    pd.DataFrame.from_dict(sip_pkt_size_ct, orient='index', columns=['sip_pkt_size_ct'])], axis=1)\n",
    "            sip_df.to_csv(path_or_buf=path+'/{}_sip.csv'.format(current_time), index=True)\n",
    "            \n",
    "            dip_df = pd.concat([pd.DataFrame.from_dict(dip_ct, orient='index', columns=['dip_ct']),\n",
    "                    pd.DataFrame.from_dict(dip_pkt_size_ct, orient='index', columns=['dip_pkt_size_ct'])], axis=1)\n",
    "            dip_df.to_csv(path_or_buf=path+'/{}_dip.csv'.format(current_time), index=True)\n",
    "            # Entropy\n",
    "            entropy_sip.append(calculate_entropy(sip_ct))\n",
    "            entropy_dip.append(calculate_entropy(dip_ct))\n",
    "            # F0, F1, F2 values\n",
    "            ## src\n",
    "            sip_F0_values.append([len(sip_ct.keys())])\n",
    "            sip_F1_values.append(sum(sip_ct.values()))\n",
    "            sip_F2_values.append(sum([count**2 for count in sip_ct.values()]))\n",
    "            ## dst\n",
    "            dip_F0_values.append([len(dip_ct.keys())])\n",
    "            dip_F1_values.append(sum(dip_ct.values()))\n",
    "            dip_F2_values.append(sum([count**2 for count in dip_ct.values()]))\n",
    "            # Weibull parameters\n",
    "            sip_weibull_k, sip_weibull_theta = calculate_weibeull_paramter(sip_ct)\n",
    "            dip_weibull_k, dip_weibull_theta = calculate_weibeull_paramter(dip_ct)\n",
    "            # Reset\n",
    "            sip_ct, dip_ct = Counter(), Counter()\n",
    "            sip_pkt_size_ct, dip_pkt_size_ct = Counter(), Counter()\n",
    "            sip_df, dip_df = None, None\n",
    "            start_time = current_time\n",
    "\n",
    "# Dump total feature\n",
    "path = './results/{}/{}s'.format(pkt_pcap_filename, time_interval)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "osip_df = pd.DataFrame({'sip_F0': sip_F0_values,\n",
    "                        'sip_F1': sip_F1_values,\n",
    "                        'sip_F2': sip_F2_values,\n",
    "                        'sip_entropy': entropy_sip,\n",
    "                        'sip_weibull_k': sip_weibull_k,\n",
    "                        'sip_weibull_theta': sip_weibull_theta})\n",
    "osip_df.to_csv(path_or_buf=path+'/{}_sip_features.csv', index=False)\n",
    "odip_df = pd.DataFrame({'dip_F0': dip_F0_values,   \n",
    "                        'dip_F1': dip_F1_values,\n",
    "                        'dip_F2': dip_F2_values,\n",
    "                        'dip_entropy': entropy_dip,\n",
    "                        'dip_weibull_k': dip_weibull_k,\n",
    "                        'dip_weibull_theta': dip_weibull_theta})\n",
    "odip_df.to_csv(path_or_buf=path+'/{}_dip_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2377186, 2377186)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sum([sip_ct[count]**2 for count in sip_ct])\n",
    "b = sum([count**2 for count in sip_ct.values()])\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/Merged_MAWI19040918+CAIDAdual_oneway/1.0s/{}_dip_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dip_F0'] = df['dip_F0'].apply(lambda x: eval(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./results/Merged_MAWI19040918+CAIDAdual_oneway/1.0s/total_dip_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
